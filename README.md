# Actorâ€“Critic MCP

Actorâ€“Critic loop for coding agents, consolidating knowledge graph memory & sequential thinking into more useful tooling.

## Why this exists

Modern coding agents forget what they wrote a few minutes ago **and** canâ€™t trace which early design choice broke the build four moves later.

Thatâ€™s two separate failures:

| Layer                  | Failure                                      | Symptom set                                                               |
| ---------------------- | -------------------------------------------- | ------------------------------------------------------------------------- |
| **Memory / Retrieval** | Context falls out of scope                   | forgotten APIs â€¢ duplicated components â€¢ dead code â€¢ branch drift         |
| **Credit Assignment**  | Model canâ€™t link early moves to late rewards | oscillating designs â€¢ premature optimisations â€¢ misâ€‘prioritised refactors |

LLMs _learn_ with actorâ€“critic loops that solve temporal difference and credit assignment problems, but tool builders drop that loop at inference time.

Actorâ€“Criticâ€¯MCP attempts to bring it back **and** closes the memory hole.

Keep in mind: Memory issues â‰  Temporalâ€‘Difference issues

---

### AI Agent Context Gaps

Here is a short catalogue of problems I have encountered pair-programming with AI agents.

#### **Memory loss & retrieval**

1. **Forgetting prior definitions** â€“ APIs, schemas, or components fall outside the window and get reinvented.
2. **Rules / guidelines ignored** â€“ Rule files (`.rules.md`, ESLint, naming docs) are rarely pulled into context or linked to reward, so conventions drift.
3. **Unstructured or missing summaries** â€“ Older work isnâ€™t compacted, so longâ€‘range reasoning decays.
4. **No proactive retrieval** â€“ Tools like `resume` or `search_plan` arenâ€™t invoked, leaving blind spots.
5. **Forgetting exploration outcomes** â€“ Rejected ideas resurface; time is wasted on dÃ©jÃ â€‘vu fixes.
6. **Buried open questions** â€“ â€œTBDâ€ items never resurface, so design gaps remain unresolved.

#### **Consistency & integrity drift**

7. **Component duplication / naming drift** â€“ Same concept, new name; specs splinter.
8. **Weak code linkage** â€“ Thoughts arenâ€™t tied to artifacts; the agent doubts or overwrites its own work.
9. **Dead code & divergence from plan** â€“ Unused files linger; implementation strays from intent.
10. **Poor hygiene routines** â€“ No systematic search/reuse/tag cycle â†’ metadata rot.
11. **Loss of intent hierarchy** â€“ Downstream tasks optimise locally and break upstream goals.
12. **Stale assumptions** â€“ New requirements donâ€™t invalidate old premises; bad foundations spread.
13. **Branch divergence without reconciliation** â€“ Parallel fixes never merge; logic conflicts.
14. **No crossâ€‘branch dependency tracking** â€“ Change auth model here, tests fail over there.

Have more problems to add? File an issue to suggest adding to the list.

## What this project is

```mermaid
%% Actorâ€“Critic MCP highâ€‘level flow
graph TD
  A[Caller Agent Copilot, Cursor, ...] --> B[MCP Server]
  B --> C[Knowledge-Graph Memory]
  B --> D[Actor]
  D <--> E[Critic]
  D --> C
  E --> C
```

Highâ€‘level flow: caller â†’ MCP â†’ KG + Actor/Critic loop

- **Coding Agent**  
  Calls the actor critic mcp
- **Knowledge Graph**  
  Compact summaries + full artefacts; fast semantic lookup; survives crashes.
- **Actor**  
  Generates the next code / plan node. Writes links into the graph.
- **Critic**  
  Scores each node against longâ€‘horizon goals; updates value estimates; can veto or request revision.
- **Hotâ€‘Context Stream**  
  Only the freshest, highestâ€‘value nodes return to the LLM to keep within token budgets.

## Current status

| Area                                | State          |
| ----------------------------------- | -------------- |
| KG schema & basic mcp tools         | âœ” working      |
| Sequential thinker (Actor v0)       | âœ” working      |
| Basic Critic with ruleâ€‘based reward | ðŸš§ in progress |

See **[`notes/next_steps.md`](notes/next_steps.md)** for detail.

I am still developing & testing this out in my workflows.

After I put some more miles on it, I will add the quick start instructions to this README.

## Background

While the context gap is not directly a temporal difference problem, it lends itself to the concepts of temporal difference and credit assignment.
So it is helpful to understand these concepts in order to solve the context gaps.

**TLDR;**

TD = delayed reward.  
Credit assignment = which earlier step deserves the reward.  
Actorâ€“Critic solves both: Actor acts, Critic scores, value propagates back.

### What is a temporal difference problem?

An example: when an AI wins or loses a game of chess, it might not remember the specific moves it made earlier in the game that led to that outcome. This is a temporal difference problem, the result comes much later than the decisions that influenced it.

This ties directly into another concept: the credit assignment problem.

---

**What is the credit assignment problem?**

To build on the example above: how does the AI figure out which specific moves actually contributed to the win or loss? Not all moves mattered equally. The AI needs to assign credit (or blame) across a timeline of actions.

---

**How do the two connect?**

The temporal difference problem is about **delayed consequences**. Decisions made now might not show their results until much later.
The credit assignment problem is about **figuring out which of those decisions mattered most** when the result finally does come.

Together, they form one of the most challenging problems in long-horizon reasoning.

---

**How was this solved?**

This was a sticky problem for a long time, but one of the more effective approaches turned out to be the actorâ€“critic model.

Hereâ€™s how it works:

- The **actor** is responsible for making decisions (moves, in the chess example).
- The **critic** provides feedback. It evaluates whether those decisions seem likely to lead to a good outcome.
- If the critic believes the actorâ€™s move is good, the actor continues. If not, the actor tries a better move.

Over time, the actor learns which moves tend to lead to good results, even if the payoff comes much later. This model helps the AI assign value to intermediate steps, instead of only learning from the final outcome.

So for our purposes, the actor is the coding agent, and the critic is made available via an MCP. I am hoping to figure out how we might tie rewards back to agent moves(ie; code gen).

---

### â€¯License & contributing

This project is entirely experimental. Use at your own risk. & do what you want with it.

MIT see [license](LICENSE)
